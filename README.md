# ğŸ“¦ Tam Entegre RAG Chatbot (Gemini + LangChain + Pinecone + Streamlit)

Bu proje, Google Gemini LLM API'yi kullanarak bir RAG (Retrieval-Augmented Generation) tabanlÄ± chatbot uygulamasÄ±dÄ±r. LangChain, Pinecone ve Streamlit teknolojilerini entegre biÃ§imde iÃ§erir.

## ğŸ“ README

### ğŸš€ Proje Ã–zellikleri
- **LangChain** ile vektÃ¶r tabanlÄ± dokÃ¼man arama
- **Gemini-Pro** ile doÄŸal dil iÅŸleme
- **Pinecone** ile vektÃ¶r veritabanÄ±
- **Streamlit** ile interaktif web arayÃ¼zÃ¼

---

### ğŸ“ Dizin YapÄ±sÄ±
```
chatbot-rag-gemini/
â”œâ”€â”€ run.py               â†’ Ana uygulama dosyasÄ±
â”œâ”€â”€ requirements.txt    â†’ Gerekli Python kÃ¼tÃ¼phaneleri
â”œâ”€â”€ .env                â†’ API anahtarlarÄ± (yÃ¼klenmez)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cleaned_products_catalog.csv
```

---

### ğŸ” .env DosyasÄ±
Proje kÃ¶k dizinine `.env` dosyasÄ±nÄ± ÅŸu iÃ§erikle oluÅŸtur:
```
GOOGLE_API_KEY=senin_gemini_keyin
PINECONE_API_KEY=senin_pinecone_keyin
```

> Bu dosya `.gitignore` iÃ§ine alÄ±nmalÄ± ve GitHub'a yÃ¼klenmemelidir.

---

### âš™ï¸ requirements.txt
```
streamlit
langchain
google-generativeai
git+https://github.com/langchain-ai/langchain.git@v0.1.16#subdirectory=libs/community
pinecone
python-dotenv
pandas
```

---

### âš ï¸ Ã–nemli UyarÄ± (Gemini HatasÄ± HakkÄ±nda)

Projede sÄ±k karÅŸÄ±laÅŸÄ±lan bir hata ÅŸudur:
```
google.api_core.exceptions.NotFound: 404 models/gemini-pro is not found for API version v1beta
```
Bu hatanÄ±n nedeni, `langchain_google_genai` ve `google-generativeai` kullanÄ±lÄ±rken **model adÄ±nÄ±n yanlÄ±ÅŸ formatta** verilmesidir.

**YanlÄ±ÅŸ kullanÄ±m:**
```python
ChatGoogleGenerativeAI(model="models/gemini-pro")
```

**DoÄŸru kullanÄ±m:**
```python
ChatGoogleGenerativeAI(model="gemini-pro")
```
AynÄ± ÅŸekilde embedding kÄ±smÄ±nda da:
```python
GoogleGenerativeAIEmbeddings(model="embedding-001")
```
ÅŸeklinde yazÄ±lmalÄ±dÄ±r. `models/` Ã¶neki **gereksizdir ve hataya neden olur.**

---

### â–¶ï¸ Projeyi Ã‡alÄ±ÅŸtÄ±rmak
```bash
pip install -r requirements.txt
streamlit run run.py
```

---

### ğŸ“¤ GitHub'a YÃ¼kleme Ä°puÃ§larÄ±
1. `.gitignore` dosyasÄ±na `.env` ve `__pycache__/` gibi klasÃ¶rleri eklemeyi unutma.
2. README.md ve requirements.txt ile birlikte yÃ¼kle.
3. YÃ¼klemeden Ã¶nce `git init`, `git remote add`, `git push` iÅŸlemleri ile baÄŸla.

---

### ğŸ“Œ Ekstra Not
Bu proje demo amaÃ§lÄ±dÄ±r. Daha bÃ¼yÃ¼k veri kÃ¼meleri, cache optimizasyonlarÄ±, API sÄ±nÄ±rlarÄ± ve kimlik doÄŸrulama gibi geliÅŸmiÅŸ gÃ¼venlik Ã¶nlemleri iÃ§in geniÅŸletilebilir.

---

## ğŸ”§ Kod YapÄ±sÄ±
```python
# Ortam DeÄŸiÅŸkenlerini YÃ¼kle
load_dotenv()

# Pinecone AyarlarÄ±
INDEX_NAME = "my-index"
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
...

# Embedding
embedder = GoogleGenerativeAIEmbeddings(model="embedding-001")
...

# LLM modeli
model = ChatGoogleGenerativeAI(model="gemini-pro")
```

Proje, `streamlit` arayÃ¼zÃ¼ Ã¼zerinden kullanÄ±cÄ±dan gelen sorulara Gemini destekli cevaplar Ã¼retir.

---

### ğŸ“ YardÄ±m veya Geri Bildirim
Herhangi bir hata ya da katkÄ± iÃ§in lÃ¼tfen GitHub Ã¼zerinden [Issue](https://github.com/) aÃ§.

---

### ğŸ“Œ Not
Bu README dosyasÄ± hem teknik dokÃ¼mantasyon hem de yÃ¼kleme rehberi olarak dÃ¼zenlenmiÅŸtir.

---

## Kod (run.py)

```python
import os
import pandas as pd
import time
import streamlit as st
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from langchain_community.vectorstores import Pinecone as PineconeVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

# Ortam DeÄŸiÅŸkenlerini YÃ¼kle
load_dotenv()

# Pinecone AyarlarÄ±
INDEX_NAME = "my-index"
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
spec = ServerlessSpec(cloud="aws", region="us-east-1")

if INDEX_NAME not in [i["name"] for i in pc.list_indexes()]:
    pc.create_index(name=INDEX_NAME, dimension=768, metric="dotproduct", spec=spec)

index = pc.Index(INDEX_NAME)
time.sleep(1)

# Veri ve Embedding
data = pd.read_csv("data/cleaned_products_catalog.csv")
texts = data["Description"].tolist()
metadatas = data[["ProductID", "ProductName"]].to_dict(orient="records")

embedder = GoogleGenerativeAIEmbeddings(model="embedding-001")
embeddings = embedder.embed_documents(texts)

vectors = [
    (str(metadatas[i]["ProductID"]), embeddings[i], metadatas[i])
    for i in range(len(embeddings))
]
index.upsert(vectors=vectors)

# Vector Store
vectorstore = PineconeVectorStore(index=index, embedding=embedder, text_key="Description")

# Streamlit BaÅŸlat
st.set_page_config(page_title="RAG Chatbot", layout="wide")
st.title("ğŸ¤– RAG Chatbot (Gemini ile)")

if "history" not in st.session_state:
    st.session_state.history = []

def get_context(query):
    docs = vectorstore.similarity_search(query, k=3)
    return "\n".join([d.page_content for d in docs])

def generate_answer(question):
    context = get_context(question)
    prompt = f"""
    AÅŸaÄŸÄ±daki baÄŸlam bilgilerine dayanarak kullanÄ±cÄ± sorusunu yanÄ±tla:
    Context:
    {context}

    Soru: {question}
    Cevap:
    """
    model = ChatGoogleGenerativeAI(model="gemini-pro")
    response = model.invoke(prompt)
    return response.content.strip()

query = st.text_input("Soru yaz:")

if st.button("GÃ¶nder") and query:
    yanit = generate_answer(query)
    st.session_state.history.append(("Sen", query))
    st.session_state.history.append(("Bot", yanit))

for kisi, mesaj in st.session_state.history:
    st.write(f"**{kisi}:** {mesaj}")
```
